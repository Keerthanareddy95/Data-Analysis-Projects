{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOq6oPpG19NJMXkpvBaub+J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "SvUJIkNKs5JN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000  # Only consider the top 10,000 words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hcco-Ys_7c_",
        "outputId": "f8e2c6d8-21bb-4570-b697-9f22a79b9b55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_train and x_test are the reviews, and y_train and y_test are the labels (0 for negative, 1 for positive)"
      ],
      "metadata": {
        "id": "i-xgWZZPnHUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "QThUqSs_ACQE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))  # Embedding layer to learn word representations\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))  # LSTM layer with 64 units\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification"
      ],
      "metadata": {
        "id": "LJPR_OmEADsv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding Layer: Transforms the integer-encoded words into dense vectors of fixed size (128 in this case). This layer essentially \"learns\" meaningful representations for words during training.\n",
        "- max_features: The number of unique words (10,000 most frequent words in this case).\n",
        "- 128: The dimension of the word embeddings (i.e., the size of the dense vector representing each word)."
      ],
      "metadata": {
        "id": "k9dp8khwnmNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LSTM Layer: The LSTM will process the sequence of word vectors to capture the relationships between words over time.\n",
        "- 64: The number of LSTM units (or neurons) in this layer, controlling how much information can be remembered at a time.\n",
        "- dropout=0.2: A regularization technique to prevent overfitting. It randomly drops 20% of the neurons during training.\n",
        "- recurrent_dropout=0.2: This adds dropout to the recurrent connections in the LSTM, meaning the connections between timesteps are also regularized."
      ],
      "metadata": {
        "id": "XreVQoD2oFlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dense Layer: This is the output layer. It is fully connected (dense) to the LSTM layer and has one unit (neuron), meaning it outputs a single value.\n",
        "- Since this is a binary classification problem (positive or negative sentiment), we use the sigmoid activation function to output a value between 0 and 1.\n",
        "- A value closer to 0 will indicate a negative review, and a value closer to 1 will indicate a positive review."
      ],
      "metadata": {
        "id": "OZTa_vOroZWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "exJMvEc9AGAy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=3,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LFTa-IlAovS",
        "outputId": "315a9ea0-8fb0-4031-e35d-f086c49540f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 190ms/step - accuracy: 0.7141 - loss: 0.5430 - val_accuracy: 0.8267 - val_loss: 0.4018\n",
            "Epoch 2/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 188ms/step - accuracy: 0.8559 - loss: 0.3461 - val_accuracy: 0.8145 - val_loss: 0.4138\n",
            "Epoch 3/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 187ms/step - accuracy: 0.8805 - loss: 0.3015 - val_accuracy: 0.8444 - val_loss: 0.3632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78f5ddf6cac0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkZle_HHA0sf",
        "outputId": "08e78b7c-e55b-4caf-d770-1622365d3923"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.8421 - loss: 0.3687\n",
            "Test score: 0.36324647068977356\n",
            "Test accuracy: 0.8443599939346313\n"
          ]
        }
      ]
    }
  ]
}